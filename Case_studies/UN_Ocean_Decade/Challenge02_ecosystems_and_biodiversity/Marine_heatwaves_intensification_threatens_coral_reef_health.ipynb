{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/tools/frameworks/-/raw/main/img/UN_decade_banner.png' align='right' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../../../Index.ipynb\"><< Index</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus Marine Training Service**</font> <br>\n",
    "**Copyright:** 2022 EUMETSAT <br>\n",
    "**License:** MIT <br>\n",
    "**Authors:** B. Loveday (Innoflair UG / EUMETSAT), Hayley Evers-King (EUMETSAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "  <div style=\"width:100%\">\n",
    "    <div style=\"float:left\"><a href=\"https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.eumetsat.int%2Feumetlab%2Foceans%2Focean-training%2Fapplications%2Focean-case-studies/HEAD?urlpath=%2Ftree%2FCase_studies%2FUN_Ocean_Decade%2FChallenge02_ecosystems_and_biodiversity%2FMarine_heatwaves_intensification_threatens_coral_reef_health.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Open in Binder\"></a></div>\n",
    "    <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "  </div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>Ocean case studies</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>PREREQUISITES </b>\n",
    "\n",
    "This notebook has the following prerequisites:\n",
    "- **<a href=\"https://eoportal.eumetsat.int/\" target=\"_blank\">A EUMETSAT Earth Observation Portal account</a>** to download data from the EUMETSAT Data Store\n",
    "- **<a href=\"https://my.wekeo.eu/user-registration\" target=\"_blank\">A WEkEO account</a>** to download from WEkEO\n",
    "    \n",
    "There are no prerequisite notebooks for this module, but you may wish to look at the following notebooks on using SLSTR data; <br>\n",
    "- **<a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/sensors/learn-SLSTR\" target=\"_blank\">Learn SLSTR (EUMETSAT Gitlab)</a>**\n",
    "\n",
    "For more contextual information, users should refer to the following case study where the image we generate here is published:\n",
    "- **<a href=\"https://www.eumetsat.int/Marine-heatwave-intensification-threatens-coral-reef-health\" target=\"_blank\">Marine heatwave intensification threatens coral reef health</a>**\n",
    "\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marine heatwave intensification threatens coral reef health\n",
    "<font color=\"#138D75\">**UN Ocean Decade Challenge 2: Protect and restore ecosystems and biodiversity**</font>\n",
    "\n",
    "### Data used\n",
    "\n",
    "| Product Description | Data Store collection ID| Product Navigator | WEkEO HDA ID | WEkEO metadata |\n",
    "|:--------------------:|:-----------------------:|:-------------:|:-----------------:|:-----------------:|\n",
    "| Sentinel-3 SLSTR level-2 | EO:EUM:DAT:0412 | <a href=\"https://navigator.eumetsat.int/product/EO:EUM:DAT:SENTINEL-3:SL_2_SST?query=SLSTR&s=advanced\" target=\"_blank\">link</a> | EO:EUM:DAT:SENTINEL-3:SL_2_WST___ | <a href=\"https://www.wekeo.eu/data?view=dataset&dataset=EO%3AEUM%3ADAT%3ASENTINEL-3%3ASL_2_WST___&initial=1\" target=\"_blank\">link</a> |\n",
    "| Global OSTIA SST (Reprocessed) | - | - | EO:MO:DAT:SST_GLO_SST_L4_REP_OBSERVATIONS_010_011 | <a href=\"https://www.wekeo.eu/data?view=dataset&dataset=EO%3AMO%3ADAT%3ASST_GLO_SST_L4_REP_OBSERVATIONS_010_011\" target=\"_blank\">link</a> |\n",
    "\n",
    "### Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know how to;\n",
    "* download SLSTR Level-2 SST products from the EUMETSAT Data Store\n",
    "* spatially plot SLSTR data for the Great Barrier Reef (GBR) region\n",
    "* download OSTIA reprocessed SST data from WEkEO using the harmonised data access (HDA) API\n",
    "* make a \"climate stripes\" plot over the GBR region\n",
    "* check for marine heat waves in the GBR region\n",
    "\n",
    "### Outline\n",
    "\n",
    "Concurrent with the past century's persistent warming of global oceans, marine heatwaves (periods of extreme regional ocean warming) have become more frequent and more extreme (Laufkötter et al., 2020). They occur in many areas around the world, from the Pacific Ocean to the Atlantic Ocean to the Mediterranean Sea and threaten marine biodiversity and its ecosystems (Smale et al., 2019). One particular ecosystem impacted by marine heatwaves, are coral reefs. These heatwaves can, for example, cause coral bleaching, coral disease outbreaks, and/or algae blooms (Roberts et al., 2019). Although coral can survive these so-called bleaching events, they become more stressed, more susceptible to diseases, and, on the long-term, subject to mortality (NOAA, 2021).\n",
    "\n",
    "In this notebook we will work through an example of how you can access near-real-time data to view current SST in a region that is often affected by marine heatwaves. We will then look at this area in a long term context using a reprocessed time series, to see how the current situation compares to historical marine heat wave episodes.\n",
    "\n",
    "This Jupyter Notebook builds on the **<a href=\"https://www.eumetsat.int/Marine-heatwave-intensification-threatens-coral-reef-health\" target=\"_blank\">Marine heatwave intensification threatens coral reef health</a>** case study, and will replicate the figures 3, 5 and 6 using Level-2 data from the Copernicus Sentinel-3 SLSTR sensor and Level-4 data from the Level-4 global OSTIA record. You can find more information on these in the links above.\n",
    "\n",
    "As part of the **<a href=\"https://www.oceandecade.org/\" target=\"_blank\">United Nations Ocean Decade</a>**, ten specific challenges are being addressed. This work, and the data underlying it, support \"Challenge 2 - Protect and restore ecosystems and biodiversity\". Data on marine heatwaves can help characterise the stressors facing marine ecosystems under climate changes, and contribute to decision making to protect, manage and restore those affected.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='TOC_TOP'></a>Contents\n",
    "\n",
    "</div>\n",
    "    \n",
    " 1. [Acquiring SLSTR data from the EUMETSAT Data Store](#section1)\n",
    " 1. [Plotting SLSTR data spatially](#section2)\n",
    " 1. [Downloading OSTIA SST data via the WEkEO HDA](#section3)\n",
    " 1. [Preparing the OSTIA data](#section4)\n",
    " 1. [Making SST-based climate stripes](#section5)\n",
    " 1. [Testing for marine heatwaves](#section6)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing all of the libraries that we need to run this notebook. If you have built your python using the environment file provided in this repository, then you should have everything you need. For more information on building environment, please see the repository **<a href=\"../../../README.md\" target=\"_blank\">README</a>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs      # a library that supports mapping and projection\n",
    "import cartopy.feature as ccf   # a cartopy extension that support adding features, e.g. coastlines\n",
    "import datetime                 # a library that allows us to work with dates and times\n",
    "import glob                     # a package that helps with file searching\n",
    "import json                     # a library that helps us make JSON format files\n",
    "import matplotlib.pyplot as plt # a library the provides plotting capability\n",
    "from matplotlib import gridspec # a library the provides plotting capability\n",
    "import netCDF4 as nc            # a library the supports netCDF file read/write\n",
    "import numpy as np              # a library that lets us work with arrays; we import this with a new name \"np\"\n",
    "import os                       # a library that allows us access to basic operating system commands\n",
    "from shapely import geometry    # a library that support construction of geometry objects\n",
    "import shutil                   # a library that allows us access to basic operating system commands like copy\n",
    "import sys                      # a library that gives us access to OS system functions\n",
    "import xarray as xr             # a powerful library that helps us work efficiently with multi-dimensional arrays\n",
    "import zipfile                  # a library that allows us to unzip zip-files.\n",
    "import eumartools               # a library that helps us work with Sentinel-3 data\n",
    "import eumdac                   # a tool that helps us download via the eumetsat/data-store\n",
    "from hda import Client          # a library for downloading via wekeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following library supports the detection of marine heat waves. More information is available at the following links:\n",
    "\n",
    "- **<a href=\"https://github.com/coecms/xmhw\" target=\"_blank\">The imported package</a>**\n",
    "- **<a href=\"https://github.com/ecjoliver/marineHeatWave\" target=\"_blank\">The original development package by Eric Oliver (not imported here)</a>**\n",
    "- **<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0079661116000057?casa_token=k4Z_r0jQNNkAAAAA:Z1aQSNHpiGFC0OJMYfmCuCGCQ_DOqymmhwz0FWDChKh_nsCrm1WKGNWfsBH7CzwKpWUCfDbPXSw\" target=\"_blank\">The supporting reference (Hobday et al., 2016)</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmhw.xmhw import detect, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a directory to download our data to. We will be downloading ~3 Gb worth of Level-2 SLSTR SST products and CMEMS OSTIA reprocessed SST products. We will process some of this \"on the fly\" to reduce local space requirements. The output of this processing will be stored in the `precomputed` directory, which we will also create below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a download and precomputed directory for our SLSTR and CMEMS products\n",
    "download_dir = os.path.join(os.getcwd(), \"products\")\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "precomputed_dir = os.path.join(os.getcwd(), \"precomputed\")\n",
    "os.makedirs(precomputed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a switch that determines if we should download new data. If `download_data = False` the notebook will look for existing, locally processed data, saving us time if we run more than once. By default this is set to true, assuming that the notebook has not yet been run.\n",
    "\n",
    "We will also define a regions over which to plot our SLSTR SST products and extract our OSTIA SST data. The sub-sampling parameter allows us to reduce the SLSTR grid, which may be necessary if you are running on a memory limited environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we download new data?\n",
    "download_data = True\n",
    "\n",
    "# plot region for SLSTR data: W, E, S, N\n",
    "plot_region = [142, 156, -25, -10]\n",
    "\n",
    "# sub-sampling for SLSTR pre-processing: default = 1 implies no subsampling \n",
    "plot_data_subsample = 1\n",
    "\n",
    "# OSTIA MHW region E, S, W, N\n",
    "MHW_region = [142, -21, 152, -11]\n",
    "start_year, end_year, iter_year = [1992, 2021, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "\n",
    "## <a id='section0'></a>0. Supporting functions\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we will define one short function that manages the pre-processing of our SLSTR data. This will extract only the variables we need (sea_surface_temperature, sses_bias, quality_level) only over the region we defined above, and applying our subsampling as required. Later, we will call this function as part of our download process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_SST_granule(SST_file, region, subsample=1, quality_level=4):\n",
    "    \"\"\"\n",
    "    Quick function to process Level-2 SLSTR granules for memory management. \n",
    "    Regionally subsets, applies bias and masks out values lower than the quality level.\n",
    "\n",
    "    Args:\n",
    "        SST_file (string)        : the file to process\n",
    "        region (list)            : the area subset to use when extracting data\n",
    "        subsample (int)          : the grid subsampling parameter\n",
    "        quality_level (int)      : the quality level to flag SLSTR data at. Anything lower than this value \n",
    "                                   will be discarded\n",
    "    Returns:\n",
    "        lon (array)              : the extracted longitude\n",
    "        lat (array)              : the extracted latitude\n",
    "        bias_corr_QC_SST (array) : the extracted, bias corrected, quality flagged SST\n",
    "\n",
    "    \"\"\"\n",
    "    SST_data = xr.open_dataset(SST_file)\n",
    "    ext_x, ext_y, ext_mask = eumartools.subset_image(SST_data[\"lon\"], SST_data[\"lat\"], \n",
    "                                             [region[0], region[1], region[1], region[0]],\n",
    "                                             [region[2], region[2], region[3], region[3]],\n",
    "                                             mode='global')\n",
    "    \n",
    "    bias_corr_QC_SST = np.array(SST_data[\"sea_surface_temperature\"] + SST_data[\"sses_bias\"] - 273.15)\n",
    "    bias_corr_QC_SST[np.array(SST_data[\"quality_level\"]) < 4] = np.nan\n",
    "    \n",
    "    lon = np.squeeze(np.array(SST_data[\"lon\"]))[np.nanmin(ext_y):np.nanmax(ext_y):subsample,\n",
    "                                                np.nanmin(ext_x):np.nanmax(ext_x):subsample]\n",
    "    lat = np.squeeze(np.array(SST_data[\"lat\"]))[np.nanmin(ext_y):np.nanmax(ext_y):subsample,\n",
    "                                                np.nanmin(ext_x):np.nanmax(ext_x):subsample]\n",
    "    bias_corr_QC_SST = np.squeeze(bias_corr_QC_SST)[np.nanmin(ext_y):np.nanmax(ext_y):subsample,\n",
    "                                                    np.nanmin(ext_x):np.nanmax(ext_x):subsample]\n",
    "    SST_data.close()\n",
    "\n",
    "    return lon, lat, bias_corr_QC_SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section1'></a>1. Acquiring SLSTR data from the EUMETSAT Data Store\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to retrieve the SLSTR data from the EUMETSAT Data Store. Collections are stored according to their `collection id`, which for SLSTR Level-2 products is `EO:EUM:DAT:0412` (as specified in the *Data Used* section above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = 'EO:EUM:DAT:0412'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will also provide the relevant products we want from this collection, stored in `product_list`, which in this case is four granules corresponding to night-time SST over the eastern tropical pacific. For more information on how to search for the data you want, please see the Learn SLSTR Gitlab link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [\n",
    "'S3A_SL_2_WST____20211222T123736_20211222T141835_20211223T225541_6059_080_066______MAR_O_NT_003.SEN3',\n",
    "'S3B_SL_2_WST____20211222T115815_20211222T133915_20211223T224804_6059_060_308______MAR_O_NT_003.SEN3'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access Sentinel-3 data from the <a href=\"https://data.eumetsat.int\">EUMETSAT Data Store</a>, we will use the EUMETSAT Data Access Client (`eumdac`). If you are working with the recommended Anaconda Python distribution and used the supplied environment file (environment.yml) to build this python environment (as detailed in the README), you will already have installed this. If not, you can install eumdac using;\n",
    "\n",
    "`conda install -c eumetsat eumdac`\n",
    "\n",
    "However, you can also find the source code on the <a href=\"https://gitlab.eumetsat.int/eumetlab/data-services/eumdac\">EUMETSAT gitlab</a>. Please visit the EUMETSAT user support confluence spaces for the the <a href=\"https://eumetsatspace.atlassian.net/wiki/spaces/DSDS/overview\">Data Store</a> and <a href=\"https://eumetsatspace.atlassian.net/wiki/spaces/EUMDAC/overview\">eumdac</a> for more information.\n",
    "\n",
    "In order to allow us to download data from the Data Store via API, we need to provide our credentials. We can do this in two ways; either by creating a file called `.eumdac_credentials` in our home directory (*option 1 - recommended*) or by supplying our credentials directly in this script (*option 2*). \n",
    "\n",
    "#### Option 1: creating  `.eumdac_credentials` in our home directory\n",
    "\n",
    "For most computer systems the home directory can be found at the path \\user\\username, /users/username, or /home/username depending on your operating system.\n",
    "\n",
    "In this file we need to add the following information exactly as follows;\n",
    "\n",
    "```\n",
    "{\n",
    "\"consumer_key\": \"<your_consumer_key>\",\n",
    "\"consumer_secret\": \"<your_consumer_secret>\"\n",
    "}\n",
    "```\n",
    "\n",
    "You must replace `<your_consumer_key>` and `<your_consumer_secret>` with the information you extract from https://api.eumetsat.int/api-key/. You will need a <a href=\"https://eoportal.eumetsat.int/\">EUMETSAT Earth Observation Portal account</a> to access this link, and in order to see the information you must click the \"Show hidden fields\" button at the bottom of the page.\n",
    "\n",
    "*Note: your key and secret are permanent, so you only need to do this once, but you should take care to never share them*\n",
    "\n",
    "Make sure to save the file without any kind of extension. Once you have done this, you can read in your credentials using the commands in the following cell. These will be used to generate a time-limited token, which will refresh itself when it expires.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "Optionally, you can provide your credentials directly as follows;\n",
    "<br>\n",
    "\n",
    "    \n",
    "`token = eumdac.AccessToken((\"consumer_key\", \"consumer_secret\"))`\n",
    "\n",
    "<br> \n",
    "Note: this method is convenient in the short term, but is not really recommended as you have to put your key and secret in this notebook, and run the risk of accidentally sharing them. This method also requires you to authenticate on a notebook-by-notebook basis. To use this method, please set `file_credentials = False` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load credentials\n",
    "file_credentials = True\n",
    "\n",
    "if file_credentials == True:\n",
    "    with open(os.path.join(os.path.expanduser(\"~\"),'.eumdac_credentials')) as json_file:\n",
    "        credentials = json.load(json_file)\n",
    "        token = eumdac.AccessToken((credentials['consumer_key'], credentials['consumer_secret']))\n",
    "        print(f\"This token '{token}' expires {token.expiration}\")\n",
    "else:\n",
    "    # see below for more information\n",
    "    token = eumdac.AccessToken((\"consumer_key\", \"consumer_secret\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a token, we can create an instance of the EUMETSAT Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = eumdac.DataStore(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this instance to search for products, but in our case we already know the ones we want (as listed in `products_list`), so we can download them directly. Once we have downloaded a granule, we will also subset the file and strip out any variables we don't need to save local storage space. This will be stored in the `precomputed` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    for product in product_list:\n",
    "        selected_product = datastore.get_product(product_id=product, collection_id=collection_id)\n",
    "        print(f\"Downloading and unzipping:\\n>> {product}\")\n",
    "\n",
    "        with selected_product.open() as fsrc, open(os.path.join(download_dir, fsrc.name), mode='wb') as fdst:\n",
    "            shutil.copyfileobj(fsrc, fdst)\n",
    "\n",
    "        with zipfile.ZipFile(fdst.name, 'r') as zip_ref:\n",
    "            for file in zip_ref.namelist():\n",
    "                if file.startswith(str(product)):\n",
    "                    zip_ref.extract(file, download_dir)\n",
    "\n",
    "        # remove zip file\n",
    "        os.remove(fdst.name)\n",
    "\n",
    "        # process downloaded file\n",
    "        SST_file = glob.glob(os.path.join(download_dir, product, \"*.nc\"))[0]\n",
    "        OUT_file = os.path.join(precomputed_dir, product.replace('.SEN3','_subset.nc'))\n",
    "        lon, lat, sst = process_SST_granule(SST_file, plot_region, subsample=plot_data_subsample)\n",
    "\n",
    "        # write to new netCDF file\n",
    "        ds = xr.Dataset({\"sst\": ((\"x\", \"y\"), sst)},\n",
    "                        coords={\"lat\": ((\"x\", \"y\"), lat), \n",
    "                                \"lon\": ((\"x\", \"y\"), lon)})\n",
    "        ds.to_netcdf(OUT_file, format='NETCDF4_CLASSIC')\n",
    "\n",
    "        # remove full size download\n",
    "        shutil.rmtree(os.path.join(download_dir, product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section2'></a>2. Plotting SLSTR data spatially\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by collecting our download Level-2 SLSTR SST products..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the files\n",
    "SLSTR_files = glob.glob(os.path.join(precomputed_dir, '*.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate through the images and plot them. The box below will set up our plot and do this for us. This will be the same as figure 3 on the case study. This code cell looks complex, but most of it is related to setting up our figure and embellishing our plot. Each section has a short comment describing what is done.\n",
    "\n",
    "*Note:  Note that this routine does not perform any binning, so newer data is just overlaid. The result is NOT a Level-3 product, but a level 2 mosaic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup figure\n",
    "fig = plt.figure(figsize=(10, 12), dpi=300)\n",
    "plt.rc('font', size=14)\n",
    "\n",
    "# setup axes\n",
    "gs = gridspec.GridSpec(3, 1, height_ratios=[20,0.5,1])\n",
    "gs.update(wspace=0.01, hspace=0.01)\n",
    "\n",
    "# setup plot 1: the composite\n",
    "axes_m = plt.subplot(gs[0,0], projection=ccrs.PlateCarree())\n",
    "axes_m.set_extent(plot_region)\n",
    "\n",
    "# make the plot: we will call this as a function as it contains a 'for' loop to iterate over our SLSTR granules.\n",
    "for SLSTR_file in SLSTR_files:\n",
    "    nc_fid = nc.Dataset(SLSTR_file)\n",
    "    # plot the SST field\n",
    "    SST = nc_fid.variables[\"sst\"][:]\n",
    "    SST[SST == 0] = np.nan\n",
    "    p1 = axes_m.pcolormesh(nc_fid.variables[\"lon\"], nc_fid.variables[\"lat\"],\n",
    "                           SST, cmap=plt.cm.RdYlBu_r,\\\n",
    "                           vmin=27, vmax=30, zorder=-1)\n",
    "    nc_fid.close()\n",
    "\n",
    "# add some map final embelishments\n",
    "axes_m.coastlines(resolution='50m', color='black', linewidth=1)\n",
    "axes_m.add_feature(ccf.NaturalEarthFeature('physical', 'land', '50m',\n",
    "                                         edgecolor='k', facecolor=ccf.COLORS['land']))\n",
    "g1 = axes_m.gridlines(draw_labels = True)\n",
    "g1.top_labels = g1.right_labels = False\n",
    "g1.xlabel_style = g1.ylabel_style = {'color': 'gray'}\n",
    "\n",
    "# setup plot2: colorbar\n",
    "axes_c = plt.subplot(gs[2,0])\n",
    "cbar = plt.colorbar(p1, cax=axes_c, orientation='horizontal')\n",
    "cbar.ax.tick_params() \n",
    "cbar.set_label('SLSTR night-time SST composite, 22 Dec 2021 [$^{o}$C]')\n",
    "plt.savefig('SLSTR_composite_22122021.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section3'></a>3. Downloading OSTIA SST data via the WEkEO HDA\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To place the image above in context we'll need to look at a time series of data. For this we can access data from the Copernicus services, where multiple data sources (different satellites etc) are combined to produce data sets that cover longer time periods. In this case we are going to look at the OSTIA Sea Surface Temperature using both the NRT and reprocessed data streams, so we can look at data right up to the time period of the SLSTR images we looked at previously.\n",
    "\n",
    "Reprocessed and NRT data streams are produced separately and we must be careful when we interpret these data sets together. As time progresses, we understand better how satellite instruments perform, algorithms improve, and data is reprocessed to ensure good quality and consistency between sources. With NRT we do not have all the information we have in hindsight, particularly about instrument characterisation, but this data is vitally important for understanding events that are happening right now. So, for example, you would not use combined NRT and reprocessed data sets for long-term trend analysis, and you would want to consider NRT measurements with a higher degree of uncertainty that you might consider with reprocessed data. So whilst we can use the NRT data now to get an indication of whether this is event is looking like it might be significant, we would eventually want to use a longer term time series that have been reprocessed, to establish how unusual this is event is in a more climatic context.\n",
    "\n",
    "We will construct and submit a query to the WEkEO Harmonised Data Access API to get this data. Here we have supplied the JSON file template our specific data set, but you could edit these files to look at your own time frames/regions of interest etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "  \"datasetId\": \"EO:MO:DAT:SST_GLO_SST_L4_REP_OBSERVATIONS_010_011:METOFFICE-GLO-SST-L4-REP-OBS-SST_202003\",\n",
    "  \"boundingBoxValues\": [\n",
    "    {\n",
    "      \"name\": \"bbox\",\n",
    "      \"bbox\": \"\"\n",
    "    }\n",
    "  ],\n",
    "  \"dateRangeSelectValues\": [\n",
    "    {\n",
    "      \"name\": \"position\",\n",
    "      \"start\": \"\",\n",
    "      \"end\": \"\"\n",
    "    }\n",
    "  ],\n",
    "  \"multiStringSelectValues\": [\n",
    "    {\n",
    "      \"name\": \"variable\",\n",
    "      \"value\": [\n",
    "        \"analysed_sst\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets establish a link to the WEkEO catalogue via the ECMWF HDA adaptor library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid any limit on the size of data we request, we will download in 5-year blocks. To do this, we will take our template (defined above) and run it in a loop that updates our \"tags\" (*start/end/bbox*') with the relevant datas/areas for our requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    for START_YEAR in range(start_year, end_year, iter_year):\n",
    "        query[\"dateRangeSelectValues\"][0][\"start\"] = f\"{str(START_YEAR)}-01-01T00:00:00.000Z\"\n",
    "        query[\"dateRangeSelectValues\"][0][\"end\"] = f\"{str(START_YEAR + iter_year - 1)}-12-31T00:00:00.000Z\"\n",
    "        query[\"boundingBoxValues\"][0][\"bbox\"] = MHW_region\n",
    "        print(f'From: {query[\"dateRangeSelectValues\"][0][\"start\"]}')\n",
    "        print(f'To:   {query[\"dateRangeSelectValues\"][0][\"end\"]}')\n",
    "        c = Client()\n",
    "        matches = c.search(query)\n",
    "        matches.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The products will download to this directory, so we will then move all the downloaded products folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in glob.glob(\"*.nc\"):\n",
    "    os.rename(item, os.path.join(download_dir, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section4'></a>4. Preparing the OSTIA data\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a bit or preprocessing on the OSTIA data to make it easier to use for our purposes. We begin by opening the data, then calculate spatial averages, annual values, and then, from these an SST anomaly. The `xarray` library allows us to open all the downloaded files at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_data = xr.open_mfdataset(glob.glob(os.path.join(download_dir,'METOFFICE-GLO*.nc')))\n",
    "SST_spatial_average = SST_data.mean(dim='lat').mean(dim='lon')\n",
    "SST_annual_values = SST_spatial_average.groupby('time.year').mean('time')\n",
    "SST_anomaly = SST_annual_values[\"analysed_sst\"] - SST_annual_values[\"analysed_sst\"].mean(dim=\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section5'></a>5. Making SST-based climate stripes\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot we'll make, shows ‘stripes’ of the average sea surface temperature anomaly for our region of interest. Recently, ‘climate stripes’ have been used by 'citizen scientists' all over the world to show long-term trends in regional temperatures. The plot below shows a stripes-style graphic derived using the SST time series we have extracted. High anomalies are apparent during 2014, in 2015 during the previous marine heatwave, and were associated with reports in 2019 and 2020.\n",
    "\n",
    "First we need to do a bit of formatting to get our array in the right size for the plot we want to make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_stripes = np.squeeze(np.array(SST_anomaly))\n",
    "SST_stripes_2D = np.repeat(SST_stripes[np.newaxis,...], 2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our stripes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 7))\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "vmax = np.nanmax(abs(SST_stripes))\n",
    "\n",
    "date_ticks = []\n",
    "for ii in range(len(SST_anomaly)):\n",
    "    date_ticks.append(str(start_year+ii))\n",
    "    \n",
    "plt.pcolormesh(SST_stripes_2D, vmin=vmax*-1, vmax=vmax, cmap=plt.cm.RdBu_r)\n",
    "plt.xticks(np.arange(len(SST_anomaly))+0.5, date_ticks, rotation='90', fontsize=20)\n",
    "plt.xlim([0,len(SST_anomaly)])\n",
    "plt.yticks([],[], fontsize=12)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('SST anomaly [$^{o}$C]',fontsize=20)\n",
    "plt.savefig('Climate_stripes_GBR.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section6'></a>6. Testing for marine heatwaves\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to do a little bit of analysis to see if we can routinely identify heatwaves in our time series. We can do this using an open toolkit developed for this purpose by Hobday et al., (you can see it imported at the top of this notebook in an xarray based implementation). \n",
    "\n",
    "Initially, we need to do some formatting to get the dates for our time series in the right format for ingestion in to the toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [datetime.datetime.strptime(str(i), \"%Y-%m-%dT%H:%M:%S.%f000\").toordinal() \n",
    "           for i in np.asarray(SST_spatial_average[\"time\"])]\n",
    "\n",
    "doy = [datetime.datetime.strptime(str(i), \"%Y-%m-%dT%H:%M:%S.%f000\").timetuple().tm_yday - 1\n",
    "       for i in SST_spatial_average[\"time\"].values]\n",
    "\n",
    "dates = [datetime.datetime.fromordinal(tt) for tt in times]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the analysis, which first establishes a threshold based on the time series and then detects instances where this is exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SST_time_series = SST_spatial_average[\"analysed_sst\"].load() - 273.15\n",
    "clim = threshold(SST_time_series)\n",
    "mhws = detect(SST_time_series, clim['thresh'], clim['seas'])\n",
    "rolling_clim_seas = np.array([i for ii in clim['seas'].values[doy] for i in ii])\n",
    "rolling_clim_thresh = np.array([i for ii in clim['thresh'].values[doy] for i in ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get the toolkit to tell us how many events are present in our time series..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = len(mhws['event'])\n",
    "print(f\"Number of heatwaves: {n_events}\")\n",
    "ev = 41 # zero indexed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also plot the time series, and show where the events identified occurred..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.rc('font', size=14)\n",
    "\n",
    "p1, = plt.plot(SST_spatial_average[\"time\"], rolling_clim_seas, 'b--', linewidth=1, zorder=1)\n",
    "p2, = plt.plot(SST_spatial_average[\"time\"], rolling_clim_thresh, 'r', linewidth=1, zorder=2)\n",
    "p3, = plt.plot(SST_spatial_average[\"time\"], SST_time_series, 'k', linewidth=1, zorder=3)\n",
    "\n",
    "# Find indices for previous MHW and shade\n",
    "for ev0 in np.arange(ev-10, min(ev+10, n_events), 1):\n",
    "    t1 = np.where(SST_spatial_average[\"time\"]==mhws['time_start'][ev0].values)[0][0]\n",
    "    t2 = np.where(SST_spatial_average[\"time\"]==mhws['time_end'][ev0].values)[0][0]\n",
    "    if ev0 == ev:\n",
    "        plot_col = 'r'\n",
    "    else:\n",
    "        plot_col = (1,0.6,0.5)\n",
    "    plt.fill_between(np.array(dates[t1:t2+1]), rolling_clim_thresh[t1:t2+1], np.array(SST_time_series[t1:t2+1]), \\\n",
    "                     color=plot_col)\n",
    "\n",
    "plt.xlim(SST_spatial_average[\"time\"][-1000], SST_spatial_average[\"time\"][-1])\n",
    "plt.ylim(clim['seas'].min() - 1, clim['seas'].max() + 2)\n",
    "plt.ylabel(r'SST [$^\\circ$C]')\n",
    "plt.legend([p1, p2, p3],[\"Seasonal climatology\", \"Threshold\", \"SST\"], frameon=False)\n",
    "plt.savefig('Marine_heat_waves_GBR.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an example of the sorts of analyses that can be developed with SST data for marine heatwaves. The diversity of data available through Copernicus programme allows for the investigation of this phenomena at both the event and climate scales. To extend this analysis you could use a longer time series of data to determine climate related trends. You could also routinely compare NRT data to a climatology (with the caveat of greater uncertainty associated with the NRT data source) for any region of interest to investigate it's current MHW status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a href=\"../../../Index.ipynb\"><< Index</a>\n",
    "<hr>\n",
    "<a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/applications/ocean-case-studies\">View on GitLab</a> | <a href=\"https://training.eumetsat.int/\">EUMETSAT Training</a> | <a href=mailto:ops@eumetsat.int>Contact helpdesk for support </a> | <a href=mailto:Copernicus.training@eumetsat.int>Contact our training team to collaborate on and reuse this material</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
